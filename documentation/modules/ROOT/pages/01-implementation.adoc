image::instructlab-banner.png[]

= Laboratorio 1: InstructLab y la IA generativa

[#comprender]
== üîç Comprender el entorno

Ech√©mosle un vistazo al laboratorio. Si a√∫n no lo has hecho, oculta las instrucciones del laboratorio en ingl√©s situando el rat√≥n entre las instrucciones y los terminales y deslizando al m√°ximo a la izquierda. Por comodidad, deja el entorno en la parte derecha de tu pantalla y las instrucciones en espa√±ol a la izquierda.

En la web del entorno, vemos en la esquina superior izquierda dos pesta√±as. En "Terminals", veremos un terminal arriba y un terminal abajo. Est√°n conectados a la misma m√°quina virtual y son id√©nticos. Por lo general escribiremos en el de arriba, luego en las instrucciones se especifica cuando pasar a abajo.

En la segunda pesta√±a, leemos "Parasol". Esta es la web de nuestra empresa ficticia que queremos mejorar gracias a nuestro trabajo con InstructLab.

En las instrucciones que vamos a seguir, ver√°s rect√°ngulos naranjas que incluyen comandos o frases. En ellos, puedes clicar en el icono de la derecha para directamente copiar esos fragmentos. ¬°Algo √∫til para facilitar el laboratorio!

[#uso-basico]
== 1. Interactuar con un modelo

Nuestra primera tarea ser√° ofrecer en nuestra web un chatbot al que hacer preguntas sobre siniestros de coches. ¬°Calma! Tenemos toda la interfaz gr√°fica y un modelo de lenguaje listo para hacerlo. En este caso, usaremos InstructLab para servir un modelo y as√≠ hacerle llamadas desde nuestra web.

=== Preparaci√≥n del entorno

Para utilizar InstructLab, debemos de cumplir con estos requisitos:

* Un sistema Linux.
* Python 3.10 o 3.11.
* Un m√≠nimo de 250GB de espacio en el disco.
* Compilador de C + +.

No hay que preocuparse por ello ahora mismo, el laboratorio cumple con todos esos requisitos.

=== Ejecutar InstructLab

Lo primero que tenemos que hacer es crear un entorno virtual Python que nos permitir√° interactuar con la l√≠nea de comandos de InstructLab. Escribe los siguientes comandos en el terminal superior:

Dirigete hasta la carpeta predefinida InstructLab y activa el entorno virtual python ejecutando los siguientes comandos:

[.console-input]
[source,bash]
----
cd ~/instructlab
source venv/bin/activate
----

El terminal deber√≠a parecerse a esto:

[source,bash]
----
(venv) [instruct@bastion instructlab]$
----

De esta forma, ya podemos usar InstructLab con el comando `ilab` desde el entorno virtual de Python. Verif√≠calo con este comando:

[.console-input]
[source,bash]
----
ilab
----

Deber√≠as obtener esto:

[source,bash]
----
Usage: ilab [OPTIONS] COMMAND [ARGS]...


  CLI for interacting with InstructLab.


  If this is your first time running ilab, it`s best to start with `ilab config init`
  to create the environment.


Options:
  --config PATH  Path to a configuration file.  [default: /home/instruct/.config/instructlab/config.yaml]
  -v, --verbose  Enable debug logging (repeat for even more verbosity)
  --version      Show the version and exit.
  --help         Show this message and exit.

Commands:
  config    Command Group for Interacting with...
  data      Command Group for Interacting with...
  model     Command Group for Interacting with...
  system    Command group for all system-related...
  taxonomy  Command Group for Interacting with...

Aliases:
  chat      model chat
  generate  data generate
  serve     model serve
  train     model train
----

¬°Perfecto! Estamos listos para meternos de lleno con InstructLab.

=== Configurar InstructLab

Para empezar a trabajar, InstructLab necesita *inicializar*. El comando `ilab config init` se encarga de:

* Crear una estructura de directorios para los modelos, datos generados, etc..
* Establecer una configuraci√≥n y taxonom√≠a inicial (estructura de conocimientos y habilidades).
* Detectar la CPU y GPU de nuestro sistema.

Introduce el siguiente comando:

[.console-input]
[source,bash]
----
ilab config init
----

Deber√≠as recibir el siguiente output:

[source,bash]
----
Welcome to InstructLab CLI. This guide will help you to setup your environment.
Please provide the following values to initiate the environment [press Enter for defaults]:
Path to taxonomy repo [/home/instruct/.local/share/instructlab/taxonomy]:
----

En este caso, pulsaremos la tecla ENTER. Eso har√° que usemos una taxonom√≠a por defecto. M√°s adelante explicamos qu√© son las taxonom√≠as.

El comando continuar√° y lo siguiente que ense√±ar√° es esto:

[source,bash]
----
Path to your model [/home/instruct/.cache/instructlab/models/merlinite-7b-lab-Q4_K_M.gguf]:
Generating 
/home/instruct/.config/instructlab/config.yaml
...
Detecting Hardware...
We chose Nvidia 1x L4 as your designated training profile. This is for systems with 24 GB of vRAM.
This profile is the best approximation for your system based off of the amount of vRAM. We modified it to match the number of GPUs you have.
Is this profile correct? [Y/n]: Y
----

El comando ha detectado que estamos usando un hardware espec√≠fico de Nvidia con 24GB de VRAM, as√≠ que ha adaptado el funcionamiento de InstructLab para sacar todo el potencial de nuestro sistema.

Ahora, pulsaremos la tecla Y y luego ENTER.

Si todo ha ido bien, el comando terminar√° avis√°ndonos de que la inicializaci√≥n ha sido completada con √©xito.

[source,bash]
----
'Initialization completed successfully, you`re ready to start using ilab. Enjoy!'
----

Durante la fase de inicializaci√≥n pasan varias cosas. Se crea una taxonom√≠a por defecto en el sistema de archivos local y un archivo de configuraci√≥n (config.yaml) en el directorio ‚Äúhome/instruct/.config/instructlab/‚Äù.

El archivo config.yaml contiene los par√°metros por defecto que utilizaremos durante este laboratorio para personalizar el desempe√±o de InstructLab. Si decides experimentar con InstructLab despu√©s del evento, es importante que eches un ojo a este archivo para que puedas ajustar los par√°metros a tu gusto.


=== Descargar modelos

Con el entorno InstructLab configurado, ahora descargaremos dos modelos cuantificados (es decir, comprimidos y optimizados) en el directorio local. Estamos utilizando modelos cuantificados porque disponemos de una √∫nica GPU para este laboratorio. Para un mayor rendimiento o casos de uso en producci√≥n, se utilizar√≠an modelos sin cuantificar.

* Granite se utilizar√° como un servidor de modelos para las solicitudes de la API.
* Merlinite nos ayudar√° a crear datos sint√©ticos para entrenar un nuevo modelo.

Primero, descargaremos el modelo Granite:

[.console-input]
[source,bash]
----
ilab model download --repository instructlab/granite-7b-lab-GGUF --filename=granite-7b-lab-Q4_K_M.gguf --hf-token $HUGGINGFACE_RO_TOKEN
----

Y ahora hacemos igual para Merlinite:

[.console-input]
[source,bash]
----
ilab model download --repository instructlab/merlinite-7b-lab-GGUF --filename=merlinite-7b-lab-Q4_K_M.gguf --hf-token $HUGGINGFACE_RO_TOKEN
----

Como puedes ver, el comando ilab model download descarga los modelos desde el repositorio oficial de InstructLab en HuggingFace. 

El output despu√©s de descargar cada modelo debe parecese a esto:

[source,bash]
----
Downloading model from Hugging Face: instructlab/granite-7b-lab-GGUF@main to /home/instruct/.cache/instructlab/models...
Downloading 'granite-7b-lab-Q4_K_M.gguf' to '/home/instruct/.cache/instructlab/models/.cache/huggingface/download/granite-7b-lab-Q4_K_M.gguf.6adeaad8c048b35ea54562c55e454cc32c63118a32c7b8152cf706b290611487.incomplete'
INFO 2024-09-10 16:51:32,740 huggingface_hub.file_download:1908: Downloading 'granite-7b-lab-Q4_K_M.gguf' to '/home/instruct/.cache/instructlab/models/.cache/huggingface/download/granite-7b-lab-Q4_K_M.gguf.6adeaad8c048b35ea54562c55e454cc32c63118a32c7b8152cf706b290611487.incomplete'
granite-7b-lab-Q4_K_M.gguf: 100%|‚ñà| 4.08G/4.08G [00:19<00:00, 207
Download complete. Moving file to /home/instruct/.cache/instructlab/models/granite-7b-lab-Q4_K_M.gguf
INFO 2024-09-10 16:51:52,562 huggingface_hub.file_download:1924: Download complete. Moving file to /home/instruct/.cache/instructlab/models/granite-7b-lab-Q4_K_M.gguf
----

Ahora que los modelos est√°n descargados, podemos servir y chatear con el modelo Granite. Servir el modelo simplemente significa que vamos a ejecutar un servidor que permitir√° a otras herramientas interactuar de forma similar a hacer una llamada a la API.

=== Servir un modelo

Vamos a servir el modelo Granite con el siguiente comando:

[.console-input]
[source,bash]
----
ilab model serve --model-path /home/instruct/.cache/instructlab/models/granite-7b-lab-Q4_K_M.gguf
----

Como puede ver, el comando serve puede tomar un argumento opcional --model-path. Si no se proporciona ninguna ruta de modelo, se utilizar√° el valor predeterminado del archivo config.yaml.

Este comando en especial, tarda alrededor de unos 10, 15 segundos. Nos toca esperar un poco hasta que muestre el siguiente mensaje:

[source,bash]
----
INFO ... After application startup complete see http://127.0.0.1:8000/docs for API.
----

¬°Genial! Acabamos de servir nuestro primer modelo y estamos listos para chatear con √©l.

=== Chatear con el modelo

Ya que estamos sirviendo el modelo en el terminal superior, lo dejamos trabajando y pasamos a escribir en el terminal inferior.

Debemos volver a activar el entorno virtual Python para ejecutar el comando ilab chat y comunicarnos con el modelo que est√° sirviendo.

[.console-input]
[source,bash]
----
cd ~/instructlab
source venv/bin/activate
----

En tu terminal deber√≠a aparecer:

[source,bash]
----
(venv) [instruct@bastion instructlab]$
----

Ya podemos volver a utilizar InstructLab. En este caso usaremos el comando ilab chat.

[.console-input]
[source,bash]
----
ilab model chat -m /home/instruct/.cache/instructlab/models/granite-7b-lab-Q4_K_M.gguf
----

El resultado deber√≠a ser una interfaz parecida a esta:

[source,bash]
----
‚ï≠‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ïÆ
‚îÇ Welcome to InstructLab Chat w/ MODELS/GRANITE-7B-LAB-Q4_K_M.GGUF
‚ï∞‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ïØ
>>>
----

Tenemos todo listo para hacerle preguntas a nuestro LLM. Vamos a ver si conoce qu√© es Openshift, prueba a escribir:

[.console-input]
[source,bash]
----
What is OpenShift in 20 words or less?
----

¬°Genial! El modelo responde correctamente y deber√≠a explicar que Openshift es una plataforma de contenerizaci√≥n desarrollada por Red Hat.

Puedes continuar haci√©ndole preguntas aunque ten en cuenta lo siguiente: este modelo no dispone de conexi√≥n a internet y su conocimiento, aunque es general, es limitado. ¬°Pero no hay problema! Con InstructLab lo entrenaremos para que aprenda m√°s sobre esas √°reas que nos interesan.

=== Integrar modelo en la web

Hasta ahora, hemos visto los conceptos b√°sicos de c√≥mo interactuar con InstructLab. Ahora vamos a dar un paso m√°s all√° mediante el uso de InstructLab con una aplicaci√≥n de ejemplo. Vamos a utilizar InstructLab para aprovechar el modelo Granite entren√°ndolo con nuevos conocimientos y permitiendo que responda a las preguntas con eficacia. Esto lo haremos en el contexto de Parasol, una empresa ficticia que procesa las reclamaciones de seguros.

Parasol tiene una aplicaci√≥n de chatbot con IA (el modelo Granite) para proporcionar sugerencias de reparaci√≥n para las reclamaciones presentadas. Esto permitir√≠a a Parasol agilizar la tramitaci√≥n de varias reclamaciones en espera.

¬°Vamos a poner a prueba Granite usando la web de Parasol!

Volvemos a dejar los dos terminales como est√°n y pinchamos en la pesta√±a superior "Parasol".

image::parasol-view.png[]


Lo que veremos ser√° la intefaz de la web de Parasol. Tenemos una tabla en la que cada fila es un caso de reclamaci√≥n distinto. Si tienes curiosidad, puedes tomarte un tiempo para explorar la web.

Para continuar con el laboratorio, nos centraremos en el primer caso de la tabla, el que tiene el identificador CLM195501 y ha sido generado por un tal Marty McFly.

image::parasol-claim.png[]

En la p√°gina de la reclamaci√≥n, puedes ver que tenemos informaci√≥n como: la fecha en la que ocurri√≥ el siniestro, el lugar, un resumen de c√≥mo ocurri√≥ el accidente y c√≥mo se siente el cliente.

Si miras en la esquina inferior derecha, hay un bot√≥n azul. Vamos a clicarlo para abrir el chat con el modelo Granite. Este chat est√° utilizando el modelo que hemos servido antes.

image::parasol-chat.webp[]

Vamos a imaginar que somos el personal de Parasol que gestiona las reclamaciones y que nos gustar√≠a saber cu√°nto puede costar reparar el condensador de flujo del DeLorean de McFly.

[.console-input]
[source,bash]
----
How much does it cost to repair a flux capacitor?
----

Deber√≠as ver algo parecido a lo siguiente. Ten en cuenta que los LLM no son deterministas por naturaleza. Esto significa que incluso con la misma entrada, el modelo producir√° respuestas variables. Por lo tanto, tus resultados pueden variar ligeramente.

image::parasol-chat-response.webp[]

Lo que acabamos de hacer es proporcionar informaci√≥n contextual sobre la reclamaci√≥n en una conversaci√≥n con el LLM utilizando Prompt Engineering. Pero, por desgracia, el chatbot no sabe cu√°nto cuesta reparar un condensador de flujo, ni tendr√° ning√∫n conocimiento espec√≠fico del dominio de nuestra empresa. Con InstructLab, podemos cambiar eso ense√±ando al modelo.

[#entrenamiento]
== 2. Entrenamiento del Modelo

Hemos probado a chatear con el modelo y ahora vamos a aprovechar el potencial de InstructLab, centr√°ndonos en *mejorar la taxonom√≠a*. A√±adiremos conocimiento sobre el coche de McFly al modelo para que sepa m√°s sobre sus especificaciones y pueda responder a nuestras preguntas. 

=== Entender la taxonom√≠a

¬øTe has preguntado por qu√© InstructLab se llama as√≠?

El *m√©todo LAB* (**L**arge-scale **A**lignment for chat**B**ots) se basa en taxonom√≠as. Las taxonom√≠as son archivos YAML que contienen conocimientos y habilidades que InstructLab usa para su generaci√≥n de datos. Estas se crean manualmente y con cuidado.

InstructLab facilita el proceso de ajuste y mejora de los modelos mediante la recopilaci√≥n de dos tipos de datos: conocimientos y habilidades. Esta informaci√≥n se recoge en una taxonom√≠a de archivos YAML que se usa en el proceso de generaci√≥n de datos sint√©ticos.

En la siguiente imagen puedes ver la estructura que puede tener una taxonom√≠a. Las cajas moradas son nuestros archivos YAML, o mejor dicho QNAs (archivos de preguntas y respuestas). Si desde ah√≠ seguimos hacia las capas de arriba, vemos que los YAML est√°n organizados por su tem√°tica: finanzas, matemasticas, etc... Mientras que si continuamos hacia abajo, vemos su papel en la generaci√≥n de datos sint√©ticos y entrenamiento.

image::taxonomy.png[]

Vamos a echar un vistazo a la taxonom√≠a actual. Vuelve a la pesta√±a Terminales. En la ventana de terminal inferior donde hemos chateado, introduce 'exit' para salir de la sesi√≥n de chat y escribe el siguiente comando:

[.console-input]
[source,bash]
----
cd /home/instruct/.local/share/instructlab
tree taxonomy | head -n 20
----

Deber√≠as ver algo parecido a esto:

[source,bash]
----
taxonomy
‚îú‚îÄ‚îÄ CODE_OF_CONDUCT.md
‚îú‚îÄ‚îÄ compositional_skills
‚îÇ   ‚îú‚îÄ‚îÄ arts
‚îÇ   ‚îú‚îÄ‚îÄ engineering
‚îÇ   ‚îú‚îÄ‚îÄ geography
‚îÇ   ‚îú‚îÄ‚îÄ grounded
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ arts
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ engineering
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ geography
----

=== Modificar la taxonom√≠a

Partiendo de esta taxonom√≠a, vamos a crear un lugar en el que almacenar informaci√≥n sobre el coche de McFly. 

[.console-input]
[source,bash]
----
mkdir -p /home/instruct/.local/share/instructlab/taxonomy/knowledge/parasol/claims
----

Ahora lo que nos falta es un archivo QNA con los detalles espec√≠ficos del coche. Por suerte, tenemos un PDF con esa informaci√≥n que puedes ver en este link:https://github.com/dgpmakes/instructlab-start/blob/master/ilab/DeLorean%20PDF.pdf[link].

Siguiente paso. Vamos a utilizar Docling, una herramienta open-source que nos permite f√°cilmente transformar archivos a formatos m√°s convenientes. De esta forma, podremos transformar nuestro PDF en un archivo de texto y as√≠ crear nuesto QNA de forma c√≥moda.

Utiliza el siguiente comando para instalar Docling:

[.console-input]
[source,bash]
----
pip install docling
----

Este comando puede tardar alrededor de un minuto, debemos esperar hasta que volvamos a tener la l√≠nea de comandos activa. Mientras, puedes echarle un ojo a la link:https://docling-project.github.io/docling/[documentaci√≥n] de Docling.

Una vez instalado, ejecutamos los siguientes comandos para que Docling transforme el PDF correctamente:

[.console-input]
[source,bash]
----
cd /home/instructlab/files/
docling https://github.com/dgpmakes/instructlab-start/raw/refs/heads/master/ilab/DeLorean%20PDF.pdf
----

Si todo ha ido bien, despu√©s de unos segundos deber√≠amos volver a tener el control del terminal. Vamos a echarle un ojo al resultado final.

[.console-input]
[source,bash]
----
cat /home/instructlab/files/DeLorean%20PDF.md
----

En este caso, el trabajo de Docling es muy b√°sico. Pero, ¬øy si tuvieramos un PDF con cientos de p√°ginas? Docling es realmente √∫til a la hora de preprocesar datos en el √°mbito del aprendizaje autom√°tico.

Teniendo ya la informaci√≥n en texto plano, solo quedar√≠a crear el archivo QNA y rellenarlo con toda nuestra informaci√≥n. Para ahorrar tiempo, el archivo QNA ya est√° rellenado y lo puedes ver con el siguiente comando:

[.console-input]
[source,bash]
----
cat -av ~/files/backToTheFuture/qna.yaml /home/instruct/.local/share/instructlab/taxonomy/knowledge/parasol/claims/
----

[source,bash]
----
version: 3
domain: time_travel
created_by: Marty McFly
seed_examples:
  - context: |
      The DeLorean DMC-12 is a sports car manufactured by John DeLorean`s DeLorean Motor Company
      for the American market from 1981 to 1983. The car features gull-wing doors and a stainless-steel body.
      It gained fame for its appearance as the time machine in the ''Back to the Future'' film trilogy.
    questions_and_answers:
      - question: |
          When was the DeLorean manufactured?
        answer: |
          The DeLorean was manufactured from 1981 to 1983.
      - question: |
          Who manufactured the DeLorean DMC-12?
        answer: |
          The DeLorean Motor Company manufactured the DeLorean DMC-12.
      - question: |
          What type of doors does the DeLorean DMC-12 have?
        answer: |
          Gull-wing doors.
document_outline: |
  Details and repair costs on a DeLorean DMC-12 car.
document:
  repo: https://github.com/gshipley/backToTheFuture.git
  commit: 8bd9220c616afe24b9673d94ec1adce85320809c
  patterns:
    - data.md
----

Se trata de un archivo YAML que consiste en una lista de ejemplos de preguntas y respuestas que utilizar√° el modelo maestro Merlinite para generar los datos sint√©ticos. 

Aqu√≠ tienes una descripci√≥n de cada componente del archivo:

1. `version`: La versi√≥n del archivo qna.yaml, este es el formato del archivo utilizado para la generaci√≥n de datos sint√©ticos. El valor debe ser el n√∫mero 3.

2. `created_by`: El usuario de GitHub del creador.

3. `domain`: La categor√≠a espec√≠fica del √°rea de conocimiento.

4. `seed_examples`: Una colecci√≥n de entradas key/value.

a. `context`: Un p√°rrafo extra√≠do del documento original. Cada qna.yaml puede usar un m√°ximo de 5 bloques de contexto con un tama√±o de 500 caracteres.

b. `questions_and_answers`: El par√°metro que contiene las preguntas y respuestas.

c. `question`: Especifica una pregunta ejemplo para el modelo. Cada archivo qna.yaml necesita al menos 3 pares de preguntas y respuestas por cada contexto. Las preguntas deben tener como m√°ximo 250 caracteres.

d. `answer`: Especifica la respuesta a la pregunta. Puede contener como m√°ximo 250 caracteres.

5. `document_outline`: Un resumen del documento fuente.

6. `document`: La fuente de la cu√°l hemos extra√≠do la informaci√≥n.

a. `repo`: La URL al repositorio que contiene el documento fuente. En este caso, el documento en texto plano est√° subido en un repositorio de GitHub.

b. `commit`: El n√∫mero identificativo del commit del repositorio en el que encontrar el documento en texto plano.

c. `patterns`: Una nomenclatura para especificar los archivos de texto plano del repositorio. Cualquier patr√≥n `glob` que empiece por *, como *.md, debe entrecomillarse debido a las reglas YAML. Por ejemplo, *.md.


Lo que haremos ahora ser√° copiar el archivo qna.yaml al directorio que hemos creado en la taxonom√≠a. Puedes hacerlo con el siguiente comando:

[.console-input]
[source,bash]
----
cp -av ~/files/backToTheFuture/qna.yaml /home/instruct/.local/share/instructlab/taxonomy/knowledge/parasol/claims/
----

InstructLab nos permite validar la sint√°xis de los archivos qna.yaml antes de la generaci√≥n de datos sint√©ticos. Esto nos ayuda a verificar que no hay ning√∫n error tipogr√°fico y todo est√° en orden:

[.console-input]
[source,bash]
----
ilab taxonomy diff
----

El resultado deber√≠a parecerse a esto:

[source,bash]
----
knowledge/parasol/claims/qna.yaml
Taxonomy in /home/instruct/.local/share/instructlab/taxonomy is valid :)
----

=== Generar datos sint√©ticos

¬°Perfecto! Hemos a√±adido nuevo conocimiento en la taxonom√≠a. El siguiente paso es la parte m√°s importante de InstructLab, generar los datos sint√©ticos.

Un modelo maestro usar√° la taxonom√≠a que hemos definido para generar m√°s ejemplos de preguntas y respuestas. Cuantas m√°s preguntas y respuestas tengamos, m√°s s√≥lido ser√° el entrenamiento. Finalmente, entrenaremos al modelo con nuestras preguntas y respuestas y con lo generado sint√©ticamente.
El resultado ser√° un nuevo modelo que comprenda el conocimiento que hemos indicado. Para m√°s informaci√≥n sobre la generaci√≥n de datos sint√©ticos y entrenamiento, visita este link:https://github.com/instructlab/instructlab?tab=readme-ov-file#-creating-new-knowledge-or-skills-and-training-the-model[link].

El tiempo que toma generar datos sint√©ticos depender√° de nuestro hardware. En este laboratorio, se tarda alrededor de 7 minutos en hacer la pipeline por defecto para la generaci√≥n de datos al tener una escala de 30. Para ahorrar tiempo, vamos a cambiar el factor de escalado a 5. No es lo m√°s recomendable para un caso real, pero as√≠ podremos continuar con el laboratorio m√°s r√°pido.

Es importante subrayar que las preguntas y respuestas generadas son mayormente reformulaciones de las preguntas y respuestas que hemos escrito manualmente. Aunque parezca algo simple, esto permite al modelo reconocer m√°s f√°cilmente lo que se le pide y responder correctamente. 

Vamos a iniciar la generaci√≥n con el siguiente comando:

[.console-input]
[source,bash]
----
ilab data generate --model /home/instruct/.cache/instructlab/models/merlinite-7b-lab-Q4_K_M.gguf --sdg-scale-factor 5 --pipeline simple --gpus 1
----

Vamos a esperar alrededor de un minuto a que termine de ejecutarse y volvamos a tener control sobre el terminal. Ya la generaci√≥n de datos habr√° terminado y podremos continuar con el paso final, el entrenamiento.

=== Entrenar

Entrenar es con diferencia el proceso m√°s largo. Require de potencia computacional y de tiempo. En este entorno de laboratorio en el que contamos con gr√°ficas de Nvidia lleva *varias horas*. Por cuesti√≥n de tiempo, hemos replicado los pasos de este laboratorio para dejar ya entrenado el modelo. ¬°Como si fuera un programa de cocina!

[#interaccion]
== 3. Comprobar modelo entrenado

Vamos a comprobar si el modelo ha aprendido y responde correctamente a nuestras preguntas sobre el coche de McFly. Si a√∫n se est√° sirviendo el modelo inicial en el terminal superior, dejamos de ejecutarlo clicando en ese terminal y pulsando `CTRL`+`C`. Deber√≠amos obtener algo parecido a esto y volver a tener control sobre el terminal:

[source,bash]
----
INFO 2025-24-09 12:41:08,496 server.py:197 After application startup complete see http://127.0.0.1:8000/docs for API.
^C
Aborted!
----

Genial, ahora servimos el modelo preentrenado en el mismo terminal:

[.console-input]
[source,bash]
----
ilab model serve --model-path /home/instruct/files/summit-connect-merlinite-lab-Q4.gguf
----

Esperamos unos segundos hasta obtener la confirmaci√≥n de que se est√° sirviendo:

[.console-input]
[source,bash]
----
INFO 2024-10-20 17:24:33,497 instructlab.model.serve:136: Using model '/home/instruct/summit-connect-merlinite-lab-Q4.gguf' with -1 gpu-layers and 4096 max context size.
INFO 2024-10-20 17:24:33,497 instructlab.model.serve:140: Serving model '/home/instruct/summit-connect-merlinite-lab-Q4.gguf' with llama-cpp
INFO 2024-10-20 17:24:34,492 instructlab.model.backends.llama_cpp:232: Replacing chat template:
 {% for message in messages %}
{% if message['role'] == 'user' %}
{{ '<|user|>
' + message['content'] }}
{% elif message['role'] == 'system' %}
{{ '<|system|>
' + message['content'] }}
{% elif message['role'] == 'assistant' %}
{{ '<|assistant|>
' + message['content'] + eos_token }}
{% endif %}
{% if loop.last and add_generation_prompt %}
{{ '<|assistant|>' }}
{% endif %}
{% endfor %}
INFO 2024-10-20 17:24:34,495 instructlab.model.backends.llama_cpp:189: Starting server process, press CTRL+C to shutdown server...
INFO 2024-10-20 17:24:34,495 instructlab.model.backends.llama_cpp:190: After application startup complete see http://127.0.0.1:8000/docs for API.
----

Llega el momento de la verdad, comprobar si el modelo que hemos entrenado funciona en la web de Parasol. Vamos a cambiar de pesta√±a a "Parasol". 

¬°IMPORTANTE! Tenemos que refrescar la web del navegador para aplicar nuestros cambios.

image::parasol-view.png[]

Ahora volvemos al primer caso, el del coche de Marty McFly con n√∫mero CLM195501.

image::parasol-claim.png[]

S√≥lo queda abrir la sesi√≥n de chat con el modelo clicando en el bot√≥n azul de la esquina inferior derecha. Si ya la tienes abierta, clica en el bot√≥n `+` que aparece debajo de la caja en la que escribimos texto. De esta forma, iniciaremos un nuevo chat.

image::parasol-chat.webp[]

Vamos a preguntar al chatbot la misma pregunta que le hicimos anteriormente y comprobar qu√© ha aprendido.

[.console-input]
[source,bash]
----
How much does it cost to repair a flux capacitor?
----

¬°Yuju! La respuesta deber√≠a ser mucho mejor que la √∫ltima vez. El LLM debe ser capaz de explicar que InstructLab.

image::parasol-chat-response.webp[]

=== Conclusi√≥n

*Has terminado el laboratorio de InstructLab con √©xito!* Como peque√±o repaso, has conseguido lo siguiente:

* Servir y chatear con un LLM en nuestra web
* Preparar los datos de entrenamiento y modificar la taxonom√≠a de InstructLab
* Generar datos sint√©ticos y comprobar el desempe√±o del modelo entrenado

Si quieres continuar aprendiendo sobre el funcionamiento de InstructLab, no dudes en visitar la link:https://instructlab.ai/[web oficial]. Te dejamos tambi√©n a su  link:https://github.com/instructlab[repositorio] en GitHub

Ahora vamos a centrarnos en la parte de agentes y descubrir herramientas open-source que nos hagan su uso mucho m√°s f√°cil. ¬°Adelante!
