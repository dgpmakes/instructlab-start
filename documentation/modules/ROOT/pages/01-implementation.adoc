= Laboratorio

[#uso-basico]
== 1. Interactuar con un modelo

=== Preparación del entorno

Para comenzar, establecemos un entorno virtual de Python para usar el CLI de InstructLab ya instalado.

[.console-input]
[source,bash]
----
cd ~/instructlab
source venv/bin/activate
----

¡Perfecto! Ya podemos usar InstructLab con el comando `ilab`. Para empezar a trabajar, InstructLab necesita *inicializar* para trabajar con los modelos. El comando `ilab config init` se encarga de:

* Localizar la taxonomía por defecto (estructura de conocimientos y habilidades).
* Crear el archivo de configuración (config.yaml).

El archivo *config.yaml* contiene los valores predeterminados que nos permiten ajustar el comportamiento del modelo a nuestro gusto, como el número de CPUs.

[.console-input]
[source,bash]
----
ilab config init
----

[source,bash]
----
Path to taxonomy repo [taxonomy]:
-> Pulsa la tecla ENTER
----

[source,bash]
----
Path to your model [models/merlinite-7b-lab-Q4_K_M.gguf]:
-> Pulsa la tecla ENTER
----

El resultado final de ejecutar el comando es:

[source,bash]
----
'Initialization completed successfully, you`re ready to start using ilab. Enjoy!'
----

=== Descargar y servir modelo

Con el entorno ya configurado, podemos descargar un modelo al directorio local y servirlo para interactuar con él.

[.console-input]
[source,bash]
----
ilab model download --repository instructlab/granite-7b-lab-GGUF --filename=granite-7b-lab-Q4_K_M.gguf
ilab model serve --model-path models/granite-7b-lab-Q4_K_M.gguf
----

¡Genial! Estamos listos para probar el LLM.

=== Chatear con el modelo

Vamos a dejar el modelo sirviéndose en el terminal donde hemos trabajado y usaremos *el segundo terminal*. Volvemos a activar el entorno virtual de Python, pero esta vez iniciamos una sesión de chat con el comando `ilab model chat`

[.console-input]
[source,bash]
----
cd ~/instructlab
source venv/bin/activate
ilab model chat -m models/granite-7b-lab-Q4_K_M.gguf
----

En tu terminal debería aparecer:

[source,bash]
----
╭───────────────────────────────────────────────────────────────────────────╮
│ Welcome to InstructLab Chat w/ MODELS/GRANITE-7B-LAB-Q4_K_M.GGUF
╰───────────────────────────────────────────────────────────────────────────╯
>>>
----

Tenemos todo listo para hacerle preguntas a nuestro LLM. Prueba a escribir:

[.console-input]
[source,bash]
----
Explica qué es Openshift
----

¡Genial! Parece que el modelo funciona y responde a nuestros mensajes. 

Ahora, prueba a escribir en el chat: 

[.console-input]
[source,bash]
----
Explica qué es InstructLab
----

El modelo debería ser capaz de generar una respuesta, pero se aleja mucho de la realidad. Este error se suele denominar «*alucinación*» en el mundo de la IA. Para solucionarlo, toca pasar por el entrenamiento. ¡Manos a la obra!

[#entrenamiento]
== 2. Entrenamiento del Modelo

Usaremos el potencial de InstructLab, centrándonos en *mejorar la taxonomía*. Añadiremos conocimiento sobre InstructLab al modelo para que sepa más del proyecto y pueda responder a nuestras preguntas. 

Primero vamos a terminar la sesión de chat con el modelo. Escribe 'exit'.

[.console-input]
[source,bash]
----
exit
----

=== Entender la taxonomía

¿Te has preguntado por qué InstructLab se llama así?

El *método LAB* se basa en taxonomías.
Las taxonomías son archivos YAML que contienen conocimiento y habilidades que InstructLab usa para su generación de datos.

Echémosle un ojo a la taxonomía actual.

[source,bash]
----
taxonomy/
├── CODE_OF_CONDUCT.md
├── compositional_skills
│   ├── arts
│   ├── engineering
│   ├── geography
│   ├── grounded
│   │   ├── arts
│   │   ├── engineering
│   │   ├── geography
----

Vamos a crear un directorio en el que insertar el conocimiento sobre InstructLab.

[.console-input]
[source,bash]
----
mkdir -p ~/instructlab/taxonomy/knowledge/instructlab/overview
----

En el entorno, ya hay preparado un archivo *qna.yaml*. InstructLab usa estos archivos para enseñar a los modelos. Estos contienen preguntas y respuestas sobre algo en concreto. Aquí tienes un ejemplo:

[source,bash]
----
- answer: The mission of instructlab is to let everyone shape generative AI
    by enabling contributed updates to existing LLMs in an accessible way.
    The community welcomes all those who would like to help enable everyone
    to shape the future of generative AI.
  question: 'What is the mission of Instructlab?'
----

Copiamos el archivo *qna.yaml* preparado en el directorio que hemos creado.

[.console-input]
[source,bash]
----
cp -av ~/files/qna.yaml ~/instructlab/taxonomy/knowledge/instructlab/overview
----

Para comprobar que hemos modificado correctamente la taxonomía, escribe el siguiente comando:

[.console-input]
[source,bash]
----
ilab taxonomy diff
----

Deberías obtener lo siguiente:

[source,bash]
----
Taxonomy in /taxonomy/ is valid :)
----

¡Perfecto! Tenemos todo listo para entrenar.

=== Entrenar modelo

Entrenar lleva *varias horas* y por cuestión de tiempo, vamos a mostrarte cómo serían los pasos del entrenamiento. Más adelante usaremos un modelo ya entrenado, ¡como si fuera un programa de cocina!

*EN CONSTRUCCION*

Vamos a utilizar la taxonomía para que el LLM genere más ejemplos. Esto puede tardar un poco y depende del número de instrucciones que queramos generar. Para este workshop, pediremos que genere 5 muestras.

Primero, necesitamos parar el servidor. En la pestaña del terminal en la que se está ejecutando, pulsa `CTRL`+`C`.


[source,bash]
----
INFO 2024-05-06 18:41:08,496 server.py:197 After application startup complete see http://127.0.0.1:8000/docs for API.
^C
Aborted!
----

A continuación, usaremos Merlinite como modelo maestro a efectos de nuestra generación de datos sintéticos:

[.console-input]
[source,bash]
----
cd ~/instructlab
cp ~/.cache/instructlab/models/merlinite-7b-lab-Q4_K_M.gguf  ~/instructlab/models
ilab model serve --model-path models/merlinite-7b-lab-Q4_K_M.gguf
----

Ahora, volvemos a nuestra segunda pestaña del terminal y ejecutamos este comando:

[.console-input]
[source,bash]
----
ilab data generate --num-instructions 5
----

¡Y ahora sucede la magia! InstrucLab tardará unos minutos en generar los ejemplos.

Realmente generar 5 ejemplos no es suficiente para impactar en el despempeño de un modelo. Debido a las limitaciones de tiempo, el objetivo es simplemente mostrarte el proceso utilizando comandos reales. Lo normal sería generar 100 o incluso 1000 datos adicionales. Red Hat proporciona herramientas como RHEL AI y OpenShift AI para entrenar LLMs de producción de forma efectiva.

Una vez que haya terminado, ¡échale un vistazo a las preguntas y respuestas que ha generado en el terminal! El siguiente paso es entrenar el modelo con la habilidad actualizada. Esto se realiza con el comando `ilab train`. Sin embargo, no vamos a realizar el entrenamiento debido a limitaciones de tiempo. 

[#interaccion]
== 3. Comprobar modelo entrenado

Ya estamos listos para servir el nuevo modelo. Por cuestión de tiempo, serviremos un modelo preentrenado con 100 ejemplos en vez de 5, usando exactamente el mismo proceso que antes.

Vamos a la primera pestaña del terminal y dejamos de servir el modelo Merlinite usando `CTRL`+`C`. 

[source,bash]
----
INFO 2024-05-06 18:41:08,496 server.py:197 After application startup complete see http://127.0.0.1:8000/docs for API.
^C
Aborted!
----

Luego, servimos el modelo preentrenado:

[.console-input]
[source,bash]
----
ilab model serve --model-path models/ggml-ilab-pretrained-Q4_K_M.gguf
----

Volvemos a la segunda pestaña del terminal e iniciamos el chat con el LLM.

[.console-input]
[source,bash]
----
ilab model chat --greedy-mode -m models/ggml-ilab-pretrained-Q4_K_M.gguf
----

¡Llega la hora de la verdad! Prueba a preguntar al LLM sobre InstructLab:

[.console-input]
[source,bash]
----
¿Qué es Instructlab?
----

¡Yuju! La respuesta debería ser mucho mejor que la última vez. El LLM debe ser capaz de describir a la perfección el proyecto InstructLab.

== Conclusión

*¡Laboratorio terminado con éxito!* Esperamos que hayas disfrutado probando de primera mano el potencial de InstructLab. Como pequeño repaso, has conseguido lo siguiente:

* Chatear con un LLM
* Crear ejemplos con un LLM para entrenar el modelo
* Comprobar el desempeño del modelo entrenado

Gracias por haber dedicado tu esfuerzo y tiempo en a aprender más sobre inteligencia articial y LLMs. Para más información sobre InstructLab, ¡echa un ojo a la comunidad en Github! https://github.com/instructlab


