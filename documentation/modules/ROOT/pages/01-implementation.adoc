= Laboratorio

[#uso-basico]
== 1. Interactuar con un modelo

=== Preparación del entorno

Para utilizar InstructLab, debemos de cumplir con estos requisitos:

* Un Mac con procesador M1, M2, M3 o un sistema Linux (esta demo se ha comprobado con Fedora).
* Python 3.10 o 3.11. Las versiones superiores no están soportadas a día de hoy.
* Un mínimo de 250GB de espacio en el disco.
* Compilador de C + + (gcc gcc- c + + ).



Comenzamos en el directorio del repositorio llamado #ilab#. Vamos a establecer un entorno virtual de Python para usar el CLI de InstructLab.

[.console-input]
[source,bash]
----
python -m venv venv
source venv/bin/activate
----

El siguiente paso será instalar la línea de comandos de InstructLab.

[.console-input]
[source,bash]
----
pip install instructlab
----

¡Perfecto! Ya podemos usar InstructLab con el comando `ilab`. Para empezar a trabajar, InstructLab necesita *inicializar* para trabajar con los modelos. El comando `ilab config init` se encarga de crear:

* Una taxonomía inicial (estructura de conocimientos y habilidades).
* El archivo de configuración (config.yaml).

El archivo *config.yaml* contiene los valores predeterminados que nos permiten ajustar el comportamiento del modelo a nuestro gusto, como el número de CPUs.

[.console-input]
[source,bash]
----
ilab config init
----

[source,bash]
----
Path to taxonomy repo [taxonomy]:
-> Pulsa la tecla ENTER
----

[source,bash]
----
'taxonomy' seems to not exist or is empty. Should I clone?
-> Pulsa la tecla Y, después ENTER
----

El resultado final de ejecutar el comando es:

[source,bash]
----
'Initialization completed successfully, you`re ready to start using ilab. Enjoy!'
----


* Si trabajamos con Mac OS, debería generarse en el mismo directorio un archivo config.yaml y un directorio conteniendo una taxonomía por defecto.
* Si trabajamos con Linux, se generaran estos directorios:

image::directories.png[]

=== Descargar modelos

Estamos listos para entrenar pero nos faltan los modelos. Vamos a descargar estos dos:

* link:https://huggingface.co/dgpmakes/granite-7b-lab-Q4_K_M/resolve/main/granite-7b-lab-Q4_K_M.gguf[Modelo sin entrenar: GRANITE-7B-LAB-Q4_K_M]
* link:https://huggingface.co/dgpmakes/ggml-ilab-pretrained-Q4_K_M/resolve/main/ggml-ilab-pretrained-Q4_K_M.gguf[Modelo entrenado: GGML-ILAB-PRETRAINED-Q4_K_M]

Usaremos el modelo Granite-7B-LAB-Q4, una versión cuantificada del modelo Granite-7B-LAB que es compatible con llama.cpp y ha sido afinado usando la metodología LAB. El primer modelo no está entrenado mientras que el segundo sí.

Debemos mover los modelos a su directorio específico:

[tabs]
====
Linux::
+
--
[.console-input]
[source,bash]
----
mv ~/Downloads/granite-7b-lab-Q4_K_M.gguf ~/.cache/instructlab/models/
mv ~/Downloads/ggml-ilab-pretrained-Q4_K_M.gguf ~/.cache/instructlab/models/
----
--
Mac::
+
--
[.console-input]
[source,bash]
----
mkdir -p ~/ilab/models
mv ~/Downloads/granite-7b-lab-Q4_K_M.gguf ~/ilab/models
mv ~/Downloads/ggml-ilab-pretrained-Q4_K_M.gguf ~/ilab/models
----
--
====



=== Servir modelo

Para poder chatear con el modelo, primero necesitamos servirlo. En el mismo terminal, ejecutamos:

[tabs]
====
Linux::
+
--
[.console-input]
[source,bash]
----
ilab model serve --model-path ~/.cache/instructlab/models/granite-7b-lab-Q4_K_M.gguf
----
--
Mac::
+
--
[.console-input]
[source,bash]
----
ilab model serve --model-path models/granite-7b-lab-Q4_K_M.gguf
----
--
====

Esperamos a que el terminal muestre el siguiente mensaje:

[source,bash]
----
INFO ... After application startup complete see http://127.0.0.1:8000/docs for API.
----

¡Genial! Estamos listos para probar el LLM.

=== Chatear con el modelo

Dejamos el modelo sirviéndose en el terminal donde hemos trabajado y abriremos #un segundo terminal# en el directorio ilab. Volvemos a activar el entorno virtual de Python, pero esta vez iniciamos una sesión de chat con el comando `ilab model chat`.

[tabs]
====
Linux::
+
--
[.console-input]
[source,bash]
----
source venv/bin/activate
ilab model chat -m ~/.cache/instructlab/models/granite-7b-lab-Q4_K_M.gguf
----
--
Mac::
+
--
[.console-input]
[source,bash]
----
source venv/bin/activate
ilab model chat -m models/granite-7b-lab-Q4_K_M.gguf
----
--
====


En tu terminal debería aparecer:

[source,bash]
----
╭───────────────────────────────────────────────────────────────────────────╮
│ Welcome to InstructLab Chat w/ MODELS/GRANITE-7B-LAB-Q4_K_M.GGUF
╰───────────────────────────────────────────────────────────────────────────╯
>>>
----

Tenemos todo listo para hacerle preguntas a nuestro LLM. Prueba a escribir:

[.console-input]
[source,bash]
----
Can you give me a short summary of what Openshift is?
----

¡Genial! El modelo responde correctamente y debería explicar que Openshift es una plataforma de contenerización desarrollada por Red Hat. 

Ahora, prueba a escribir en el chat: 

[.console-input]
[source,bash]
----
Can you give me a short summary of what InstructLab is?
----

Vaya. El modelo responde que InstructLab es una plataforma educativa, algo que no es cierto y se aleja de la realidad. Este error se suele denominar «*alucinación*» en el mundo de la IA. Para solucionarlo, toca pasar por el entrenamiento. ¡Manos a la obra!

Antes de continuar, vamos a cerrar la sesión de chat con el modelo. Escribe 'exit'. 

[.console-input]
[source,bash]
----
exit
----

[#entrenamiento]
== 2. Entrenamiento del Modelo

Una vez que conocemos un poco acerca de IntructLab, vamos a usar su potencial, centrándonos en *mejorar la taxonomía*. Añadiremos conocimiento sobre InstructLab al modelo para que sepa más del proyecto y pueda responder a nuestras preguntas. 

=== Entender la taxonomía

¿Te has preguntado por qué InstructLab se llama así?

El *método LAB* (**L**arge-scale **A**lignment for chat**B**ots) se basa en taxonomías.
Las taxonomías son archivos YAML que contienen conocimientos y habilidades que InstructLab usa para su generación de datos.

Echémosle un ojo a la taxonomía actual.

[source,bash]
----
taxonomy/
├── CODE_OF_CONDUCT.md
├── compositional_skills
│   ├── arts
│   ├── engineering
│   ├── geography
│   ├── grounded
│   │   ├── arts
│   │   ├── engineering
│   │   ├── geography
----

Vemos que la taxonomía incluye conocimiento sobre artes, ingeniería, geografía... Vamos a crear un directorio en el que insertar el conocimiento sobre InstructLab. En el mismo terminal, introducimos el siguiente comando:

[tabs]
====
Linux::
+
--
[.console-input]
[source,bash]
----
mkdir -p ~/.local/share/instructlab/taxonomy/knowledge/instructlab/overview
----
--
Mac::
+
--
[.console-input]
[source,bash]
----
mkdir -p taxonomy/knowledge/instructlab/overview
----
--
====

En el directorio ilab, ya hay preparado un archivo *qna.yaml*. InstructLab usa estos archivos para enseñar a los modelos. Estos contienen preguntas y respuestas sobre algo en concreto. Aquí tienes un ejemplo:

[source,bash]
----
- answer: The mission of instructlab is to let everyone shape generative AI
    by enabling contributed updates to existing LLMs in an accessible way.
    The community welcomes all those who would like to help enable everyone
    to shape the future of generative AI.
  question: 'What is the mission of Instructlab?'
----

Tenemos ya un archivo *qna.yaml* preparado en el directorio ilab. ¡Vamos a incluirlo en el directorio que hemos creado!

[tabs]
====
Linux::
+
--
[.console-input]
[source,bash]
----
cp qna.yml ~/.local/share/instructlab/taxonomy/knowledge/instructlab/overview
----
--
Mac::
+
--
[.console-input]
[source,bash]
----
cp qna.yml taxonomy/knowledge/instructlab/overview
----
--
====


Para comprobar que hemos modificado correctamente la taxonomía, escribe el siguiente comando:

[.console-input]
[source,bash]
----
ilab taxonomy diff
----

Deberías obtener lo siguiente:

[source,bash]
----
Taxonomy in /taxonomy/ is valid :)
----

¡Perfecto! Tenemos todo listo para entrenar.

=== Entrenar modelo

Entrenar lleva *varias horas* y por cuestión de tiempo, vamos a comprobar el aprendizaje usando el modelo preentrenado.¡Como si fuera un programa de cocina!

En el entrenamiento, un modelo maestro usa la taxonomía que hemos definido para generar más ejemplos de preguntas y respuestas. Después, entrenaremos al modelo con ellos. Cuantas más preguntas y respuestas, más sólido será el entrenamiento. El resultado será un nuevo modelo que comprenda el conocimiento que hemos indicado. Para más información sobre el entrenamiento, visita este link:https://github.com/instructlab/instructlab?tab=readme-ov-file#-creating-new-knowledge-or-skills-and-training-the-model[link].

[#interaccion]
== 3. Comprobar modelo entrenado

¡Hora de probar el modelo entrenado! Vamos al primer terminal y dejamos de servir el modelo usando `CTRL`+`C`. 

[source,bash]
----
INFO 2024-05-06 18:41:08,496 server.py:197 After application startup complete see http://127.0.0.1:8000/docs for API.
^C
Aborted!
----

Luego, servimos el modelo preentrenado:

[tabs]
====
Linux::
+
--
[.console-input]
[source,bash]
----
ilab model serve --model-path ~/.cache/instructlab/models/ggml-ilab-pretrained-Q4_K_M.gguf
----
--
Mac::
+
--
[.console-input]
[source,bash]
----
ilab model serve --model-path models/ggml-ilab-pretrained-Q4_K_M.gguf
----
--
====

Esperamos unos segundos, volvemos al segundo terminal e iniciamos el chat con el LLM.



[tabs]
====
Linux::
+
--
[.console-input]
[source,bash]
----
ilab model chat -m ~/.cache/instructlab/models/ggml-ilab-pretrained-Q4_K_M.gguf
----
--
Mac::
+
--
[.console-input]
[source,bash]
----
ilab model chat --greedy-mode -m models/ggml-ilab-pretrained-Q4_K_M.gguf
----
--
====


¡Llega la hora de la verdad! Prueba a preguntar al LLM sobre InstructLab:

[.console-input]
[source,bash]
----
Can you give me a short summary of what InstructLab is?
----

¡Yuju! La respuesta debería ser mucho mejor que la última vez. El LLM debe ser capaz de explicar que InstructLab.

== Conclusión

*¡Laboratorio terminado con éxito!* Esperamos que hayas disfrutado probando de primera mano el potencial de InstructLab. Como pequeño repaso, has conseguido lo siguiente:

* Chatear con un LLM
* Modificar la taxonomía de InstructLab
* Comprobar el desempeño del modelo entrenado

Gracias por haber dedicado tu esfuerzo y tiempo en a aprender más sobre inteligencia articial y LLMs. Para más información sobre InstructLab, ¡echa un ojo a la comunidad en Github! https://github.com/instructlab


