= Guía del Laboratorio


[#objetivos]
== Objetivos del Laboratorio

*Instructlab* es una plataforma open-source que nos permite interactuar fácilmente con Modelos de Lenguaje Grandes (LLMs). En este laboratorio, vas a conseguir:

1. Interactuar con un modelo
2. Entrenar un modelo
3. Comprobar modelo entrenado

¡Vamos allá!

[#uso-basico]
== 1. Interactuar con un modelo

=== Inicialización

Para comenzar, establecemos un entorno virtual de Python con el CLI de InstructLab ya instalado.

[.console-input]
[source,bash]
----
cd ~/instructlab
source venv/bin/activate
----

¡Perfecto! Ya podemos usar InstructLab con el comando `ilab`. Ahora InstructLab necesita *inicializar* para trabajar con los modelos. El comando `ilab config init` se encarga de:

* Localizar la taxonomía por defecto.
* Crear el archivo de configuración (config.yaml).

El archivo *config.yaml* contiene los valores predeterminados que nos permiten ajustar el comportamiento del modelo a nuestro gusto, como el número de CPUs.

[.console-input]
[source,bash]
----
ilab config init
----

[.console-input]
[source,bash]
----
Path to taxonomy repo [taxonomy]:
-> Pulsa la tecla ENTER
----

[.console-input]
[source,bash]
----
Path to your model [models/merlinite-7b-lab-Q4_K_M.gguf]:
-> Pulsa la tecla ENTER
----

El resultado final de ejecutar el comando es:

[source,bash]
----
'Initialization completed successfully, you`re ready to start using ilab. Enjoy!'
----

=== Descargar y servir modelo

Con el entorno ya configurado, podemos descargar un modelo al directorio local y servirlo para interactuar con él.

[.console-input]
[source,bash]
----
ilab model download --repository instructlab/granite-7b-lab-GGUF --filename=granite-7b-lab-Q4_K_M.gguf
ilab model serve --model-path models/granite-7b-lab-Q4_K_M.gguf
----

¡Genial! Con el modelo ya servido, estamos listos para probar el LLM.

=== Chatear con el modelo

Vamos a dejar el modelo sirviéndose en el terminal donde hemos trabajado y usaremos *el segundo terminal*. Volvemos a activar el entorno virtual de Python, pero esta vez iniciamos una sesión de chat con el comando `ilab model chat`

[.console-input]
[source,bash]
----
cd ~/instructlab
source venv/bin/activate
ilab model chat -m models/granite-7b-lab-Q4_K_M.gguf
----

En tu terminal debería aparecer:

[source,bash]
----
╭───────────────────────────────────────────────────────────────────────────╮
│ Welcome to InstructLab Chat w/ MODELS/GRANITE-7B-LAB-Q4_K_M.GGUF
╰───────────────────────────────────────────────────────────────────────────╯
>>>
----

Tenemos todo listo para hacerle preguntas a nuestro LLM. Prueba a escribir:

[.console-input]
[source,bash]
----
Explica qué es Openshift
----

¡Genial! Parece que el modelo funciona y responde a nuestros mensajes. 


[#entrenamiento]
== 2. Entrenamiento del Modelo

Ahora, prueba a escribir en el chat: 

[.console-input]
[source,bash]
----
Explica qué es InstructLab
----

El modelo debería ser capaz de generar una respuesta, pero se aleja mucho de la realidad. Este error se suele denominar «*alucinación*».

Para solucionarlo, nos centraremos en *mejorar la taxonomía* añadiendo nuevo conocimiento al modelo y así sepa más sobre InstructLab. ¡Manos a la obra!

Primero vamos a terminar la sesión de chat con el modelo. Escribe 'exit'.

[.console-input]
[source,bash]
----
exit
----

=== Entender la taxonomía

¿Te has preguntado por qué InstructLab se llama así?

El *método LAB* se basa en taxonomías.
Las taxonomías son archivos YAML que contienen conocimiento y habilidades que InstructLab usa para su generación de datos.

Echémosle un ojo a la taxonomía actual.

[source,bash]
----
taxonomy/
├── CODE_OF_CONDUCT.md
├── compositional_skills
│   ├── arts
│   ├── engineering
│   ├── geography
│   ├── grounded
│   │   ├── arts
│   │   ├── engineering
│   │   ├── geography
----

Vamos a crear un directorio en el que insertar el conocimiento sobre InstructLab.

[.console-input]
[source,bash]
----
mkdir -p ~/instructlab/taxonomy/knowledge/instructlab/overview
----

Ahora incluiremos un archivo *qna.yaml*. Este contiene un cojunto de preguntas y respuestas sobre lo que queremos que aprenda el modelo.

[.console-input]
[source,bash]
----
cp -av ~/files/qna.yaml ~/instructlab/taxonomy/knowledge/instructlab/overview
----

Con InstructLab, puedes comprobar que la estructura de la taxonomía es correcta con un comando.

[.console-input]
[source,bash]
----
ilab taxonomy diff
----

Deberías obtener lo siguiente:

[source,bash]
----
Taxonomy in /taxonomy/ is valid :)
----

Hasta aquí todo bien. ¡Toca a pasar a la parte más divertida!

Vamos a utilizar la taxonomía para que el LLM genere más ejemplos. Esto puede tardar un poco y depende del número de instrucciones que queramos generar. Para este workshop, pediremos que genere 5 muestras.

Primero, necesitamos parar el servidor. En la pestaña del terminal en la que se está ejecutando, pulsa `CTRL`+`C`.


[source,bash]
----
INFO 2024-05-06 18:41:08,496 server.py:197 After application startup complete see http://127.0.0.1:8000/docs for API.
^C
Aborted!
----

A continuación, usaremos Merlinite como modelo maestro a efectos de nuestra generación de datos sintéticos:

[.console-input]
[source,bash]
----
cd ~/instructlab
cp ~/.cache/instructlab/models/merlinite-7b-lab-Q4_K_M.gguf  ~/instructlab/models
ilab model serve --model-path models/merlinite-7b-lab-Q4_K_M.gguf
----

Ahora, volvemos a nuestra segunda pestaña del terminal y ejecutamos este comando:

[.console-input]
[source,bash]
----
ilab data generate --num-instructions 5
----

¡Y ahora sucede la magia! InstrucLab tardará unos minutos en generar los ejemplos.

Realmente generar 5 ejemplos no es suficiente para impactar en el despempeño de un modelo. Debido a las limitaciones de tiempo, el objetivo es simplemente mostrarte el proceso utilizando comandos reales. Lo normal sería generar 100 o incluso 1000 datos adicionales. Red Hat proporciona herramientas como RHEL AI y OpenShift AI para entrenar LLMs de producción de forma efectiva.

Una vez que haya terminado, ¡échale un vistazo a las preguntas y respuestas que ha generado en el terminal! El siguiente paso es entrenar el modelo con la habilidad actualizada. Esto se realiza con el comando `ilab train`. Sin embargo, no vamos a realizar el entrenamiento debido a limitaciones de tiempo. 

[#interaccion]
== 3. Comprobar modelo entrenado

Ya estamos listos para servir el nuevo modelo. Por cuestión de tiempo, serviremos un modelo preentrenado con 100 ejemplos en vez de 5, usando exactamente el mismo proceso que antes.

Vamos a la primera pestaña del terminal y dejamos de servir el modelo Merlinite usando `CTRL`+`C`. Luego, servimos el modelo preentrenado:

[.console-input]
[source,bash]
----
ilab model serve --model-path models/ggml-ilab-pretrained-Q4_K_M.gguf
----

Volvemos a la segunda pestaña del terminal e iniciamos el chat con el LLM.

[.console-input]
[source,bash]
----
ilab model chat --greedy-mode -m models/ggml-ilab-pretrained-Q4_K_M.gguf
----

¡Llega la hora de la verdad! Prueba a preguntar al LLM sobre InstructLab:

[.console-input]
[source,bash]
----
¿Qué es Instructlab?
----

¡Yuju! La respuesta debería ser mucho mejor que la última vez. El LLM debe ser capaz de describir a la perfección el proyecto InstructLab.

== Conclusión

*¡Laboratorio terminado con éxito!* Esperamos que hayas disfrutado probando de primera mano el potencial de InstructLab. Como pequeño repaso, has conseguido lo siguiente:

* Chatear con un LLM
* Crear ejemplos con un LLM para entrenar el modelo
* Comprobar el desempeño del modelo entrenado

Gracias por haber dedicado tu esfuerzo y tiempo en a aprender más sobre inteligencia articial y LLMs. Para más información sobre InstructLab, ¡echa un ojo a la comunidad en Github! https://github.com/instructlab


