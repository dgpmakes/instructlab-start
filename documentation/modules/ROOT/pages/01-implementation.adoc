= Laboratorio

[#comprender]
== üîç Comprender el entorno

Ech√©mosle un vistazo al laboratorio. Si a√∫n no lo has hecho, oculta las instrucciones del laboratorio en ingl√©s situando el rat√≥n entre las instrucciones y los terminales y deslizando al m√°ximo a la izquierda. Por comodidad, deja el entorno en la parte derecha de tu pantalla y las instrucciones en espa√±ol a la izquierda.

En la web del entorno, vemos en la esquina superior izquierda dos pesta√±as. En "Terminals", veremos un terminal arriba y un terminal abajo. Est√°n conectados a la misma m√°quina virtual y son id√©nticos. Por lo general escribiremos en el de arriba, luego en las instrucciones se especifica cuando pasar a abajo.

En la segunda pesta√±a, leemos "Parasol". Esta es la web de nuestra empresa ficticia que queremos mejorar gracias a nuestro trabajo con InstructLab.


[#uso-basico]
== 1. Interactuar con un modelo

Nuestra primera tarea ser√° ofrecer en nuestra web un chatbot al que hacer preguntas sobre siniestros de coches. ¬°Calma! Tenemos toda la interfaz gr√°fica y un modelo de lenguaje listo para hacerlo. En este caso, usaremos InstructLab para servir un modelo y as√≠ hacerle llamadas desde nuestra web.

=== Preparaci√≥n del entorno

Para utilizar InstructLab, debemos de cumplir con estos requisitos:

* Un sistema Linux.
* Python 3.10 o 3.11.
* Un m√≠nimo de 250GB de espacio en el disco.
* Compilador de C + +.

No hay que preocuparse por ello ahora mismo, el laboratorio cumple con todos esos requisitos.

=== Ejecutar InstructLab

Lo primero que tenemos que hacer es crear un entorno virtual Python que nos permitir√° interactuar con la l√≠nea de comandos de InstructLab. Escribe los siguientes comandos en el terminal superior:

Dirigete hasta la carpeta predefinida InstructLab y activa el entorno virtual python ejecutando los siguientes comandos:

[.console-input]
[source,bash]
----
cd ~/instructlab
source venv/bin/activate
----

El terminal deber√≠a parecerse a esto:

[source,bash]
----
(venv) [instruct@bastion instructlab]$
----

De esta forma, ya podemos usar InstructLab con el comando `ilab` desde el entorno virtual de Python. Verif√≠calo con este comando:

[.console-input]
[source,bash]
----
ilab
----

Deber√≠as obtener esto:

[source,bash]
----
Usage: ilab [OPTIONS] COMMAND [ARGS]...


  CLI for interacting with InstructLab.


  If this is your first time running ilab, it`s best to start with `ilab config init`
  to create the environment.


Options:
  --config PATH  Path to a configuration file.  [default: /home/instruct/.config/instructlab/config.yaml]
  -v, --verbose  Enable debug logging (repeat for even more verbosity)
  --version      Show the version and exit.
  --help         Show this message and exit.

Commands:
  config    Command Group for Interacting with...
  data      Command Group for Interacting with...
  model     Command Group for Interacting with...
  system    Command group for all system-related...
  taxonomy  Command Group for Interacting with...

Aliases:
  chat      model chat
  generate  data generate
  serve     model serve
  train     model train
----

¬°Perfecto! Estamos listos para meternos de lleno con InstructLab.

=== Configurar InstructLab

Para empezar a trabajar, InstructLab necesita *inicializar*. El comando `ilab config init` se encarga de:

* Crear una estructura de directorios para los modelos, datos generados, etc..
* Establecer una configuraci√≥n y taxonom√≠a inicial (estructura de conocimientos y habilidades).
* Detectar la CPU y GPU de nuestro sistema.

Introduce el siguiente comando:

[.console-input]
[source,bash]
----
ilab config init
----

Deber√≠as recibir el siguiente output:

[source,bash]
----
Welcome to InstructLab CLI. This guide will help you to setup your environment.
Please provide the following values to initiate the environment [press Enter for defaults]:
Path to taxonomy repo [/home/instruct/.local/share/instructlab/taxonomy]:
----

En este caso, pulsaremos la tecla ENTER. Eso har√° que usemos una taxonom√≠a por defecto. M√°s adelante explicamos qu√© son las taxonom√≠as.

El comando continuar√° y lo siguiente que ense√±ar√° es esto:

[source,bash]
----
Path to your model [/home/instruct/.cache/instructlab/models/merlinite-7b-lab-Q4_K_M.gguf]:
Generating 
/home/instruct/.config/instructlab/config.yaml
...
Detecting Hardware...
We chose Nvidia 1x L4 as your designated training profile. This is for systems with 24 GB of vRAM.
This profile is the best approximation for your system based off of the amount of vRAM. We modified it to match the number of GPUs you have.
Is this profile correct? [Y/n]: Y
----

El comando ha detectado que estamos usando un hardware espec√≠fico de Nvidia con 24GB de VRAM, as√≠ que ha adaptado el funcionamiento de InstructLab para sacar todo el potencial de nuestro sistema.

Ahora, pulsaremos la tecla Y y luego ENTER.

Si todo ha ido bien, el comando terminar√° avis√°ndonos de que la inicializaci√≥n ha sido completada con √©xito.

[source,bash]
----
'Initialization completed successfully, you`re ready to start using ilab. Enjoy!'
----

Durante la fase de inicializaci√≥n pasan varias cosas. Se crea una taxonom√≠a por defecto en el sistema de archivos local y un archivo de configuraci√≥n (config.yaml) en el directorio ‚Äúhome/instruct/.config/instructlab/‚Äù.

El archivo config.yaml contiene los par√°metros por defecto que utilizaremos durante este laboratorio para personalizar el desempe√±o de InstructLab. Si decides experimentar con InstructLab despu√©s del evento, es importante que eches un ojo a este archivo para que puedas ajustar los par√°metros a tu gusto.


=== Descargar modelos

Con el entorno InstructLab configurado, ahora descargaremos dos modelos cuantificados (es decir, comprimidos y optimizados) en el directorio local. Estamos utilizando modelos cuantificados porque disponemos de una √∫nica GPU para este laboratorio. Para un mayor rendimiento o casos de uso en producci√≥n, se utilizar√≠an modelos sin cuantificar.

* Granite se utilizar√° como un servidor de modelos para las solicitudes de la API.
* Merlinite nos ayudar√° a crear datos sint√©ticos para entrenar un nuevo modelo.

Primero, descargaremos el modelo Granite:

[.console-input]
[source,bash]
----
ilab model download --repository instructlab/granite-7b-lab-GGUF --filename=granite-7b-lab-Q4_K_M.gguf --hf-token $HUGGINGFACE_RO_TOKEN
----

Y ahora hacemos igual para Merlinite:

[.console-input]
[source,bash]
----
ilab model download --repository instructlab/merlinite-7b-lab-GGUF --filename=merlinite-7b-lab-Q4_K_M.gguf --hf-token $HUGGINGFACE_RO_TOKEN
----

Como puedes ver, el comando ilab model download descarga los modelos desde el repositorio oficial de InstructLab en HuggingFace. 

El output despu√©s de descargar cada modelo debe parecese a esto:

[source,bash]
----
Downloading model from Hugging Face: instructlab/granite-7b-lab-GGUF@main to /home/instruct/.cache/instructlab/models...
Downloading 'granite-7b-lab-Q4_K_M.gguf' to '/home/instruct/.cache/instructlab/models/.cache/huggingface/download/granite-7b-lab-Q4_K_M.gguf.6adeaad8c048b35ea54562c55e454cc32c63118a32c7b8152cf706b290611487.incomplete'
INFO 2024-09-10 16:51:32,740 huggingface_hub.file_download:1908: Downloading 'granite-7b-lab-Q4_K_M.gguf' to '/home/instruct/.cache/instructlab/models/.cache/huggingface/download/granite-7b-lab-Q4_K_M.gguf.6adeaad8c048b35ea54562c55e454cc32c63118a32c7b8152cf706b290611487.incomplete'
granite-7b-lab-Q4_K_M.gguf: 100%|‚ñà| 4.08G/4.08G [00:19<00:00, 207
Download complete. Moving file to /home/instruct/.cache/instructlab/models/granite-7b-lab-Q4_K_M.gguf
INFO 2024-09-10 16:51:52,562 huggingface_hub.file_download:1924: Download complete. Moving file to /home/instruct/.cache/instructlab/models/granite-7b-lab-Q4_K_M.gguf
----

Ahora que los modelos est√°n descargados, podemos servir y chatear con el modelo Granite. Servir el modelo simplemente significa que vamos a ejecutar un servidor que permitir√° a otras herramientas interactuar de forma similar a hacer una llamada a la API.

=== Servir un modelo

Vamos a servir el modelo Granite con el siguiente comando:

[.console-input]
[source,bash]
----
ilab model serve --model-path /home/instruct/.cache/instructlab/models/granite-7b-lab-Q4_K_M.gguf
----

Como puede ver, el comando serve puede tomar un argumento opcional --model-path. Si no se proporciona ninguna ruta de modelo, se utilizar√° el valor predeterminado del archivo config.yaml.

Este comando en especial, tarda alrededor de unos 10, 15 segundos. Nos toca esperar un poco hasta que muestre el siguiente mensaje:

[source,bash]
----
INFO ... After application startup complete see http://127.0.0.1:8000/docs for API.
----

¬°Genial! Acabamos de servir nuestro primer modelo y estamos listos para chatear con √©l.

=== Chatear con el modelo

Ya que estamos sirviendo el modelo en el terminal superior, lo dejamos trabajando y pasamos a escribir en el terminal inferior.

Debemos volver a activar el entorno virtual Python para ejecutar el comando ilab chat y comunicarnos con el modelo que est√° sirviendo.

[.console-input]
[source,bash]
----
cd ~/instructlab
source venv/bin/activate
----

En tu terminal deber√≠a aparecer:

[source,bash]
----
(venv) [instruct@bastion instructlab]$
----

Ya podemos volver a utilizar InstructLab. En este caso usaremos el comando ilab chat.

[.console-input]
[source,bash]
----
ilab model chat -m /home/instruct/.cache/instructlab/models/granite-7b-lab-Q4_K_M.gguf
----

El resultado deber√≠a ser una interfaz parecida a esta:

[source,bash]
----
‚ï≠‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ïÆ
‚îÇ Welcome to InstructLab Chat w/ MODELS/GRANITE-7B-LAB-Q4_K_M.GGUF
‚ï∞‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ïØ
>>>
----

Tenemos todo listo para hacerle preguntas a nuestro LLM. Vamos a ver si conoce qu√© es Openshift, prueba a escribir:

[.console-input]
[source,bash]
----
What is OpenShift in 20 words or less?
----

¬°Genial! El modelo responde correctamente y deber√≠a explicar que Openshift es una plataforma de contenerizaci√≥n desarrollada por Red Hat.

Puedes continuar haci√©ndole preguntas aunque ten en cuenta lo siguiente: este modelo no dispone de conexi√≥n a internet y su conocimiento, aunque es general, es limitado. ¬°Pero no hay problema! Con InstructLab lo entrenaremos para que aprenda m√°s sobre esas √°reas que nos interesan.

=== Integrar modelo en la web

Hasta ahora, hemos visto los conceptos b√°sicos de c√≥mo interactuar con InstructLab. Ahora vamos a dar un paso m√°s all√° mediante el uso de InstructLab con una aplicaci√≥n de ejemplo. Vamos a utilizar InstructLab para aprovechar el modelo Granite entren√°ndolo con nuevos conocimientos y permitiendo que responda a las preguntas con eficacia. Esto lo haremos en el contexto de Parasol, una empresa ficticia que procesa las reclamaciones de seguros.

Parasol tiene una aplicaci√≥n de chatbot con IA (el modelo Granite) para proporcionar sugerencias de reparaci√≥n para las reclamaciones presentadas. Esto permitir√≠a a Parasol agilizar la tramitaci√≥n de varias reclamaciones en espera.

¬°Vamos a poner a prueba Granite usando la web de Parasol!

Dejamos a dejar los dos terminales como est√°n y pinchamos en la pesta√±a superior "Parasol".

image::parasol-view.png[]


Lo que veremos ser√° la intefaz de la web de Parasol. Veremos una tabla en la que cada fila es un caso de reclamaci√≥n distinto. Si tienes curiosidad, puedes tomarte un tiempo para explorar la web.

Para continuar con el laboratorio, nos centraremos en el primer caso de la tabla, el que tiene el identificador CLM195501 y ha sido generado por un tal Marty McFly.

image::parasol-claim.png[]

En la p√°gina de la reclamaci√≥n, puedes ver que tenemos informaci√≥n como: la fecha en la que ocurri√≥ el siniestro, el lugar, un resumen de c√≥mo ocurri√≥ el accidente y c√≥mo se siente el cliente.

Si miras en la esquina inferior derecha, hay un bot√≥n azul. Vamos a clicarlo para abrir el chat con el modelo Granite. Este chat est√° utilizando el modelo que hemos servido antes.

image::parasol-chat.webp[]

Vamos a imaginar que somos el personal de Parasol que gestiona las reclamaciones y que nos gustar√≠a saber cu√°nto puede costar reparar el condensador de flujo del DeLorean de McFly.

[.console-input]
[source,bash]
----
How much does it cost to repair a flux capacitor?
----

Deber√≠as ver algo parecido a lo siguiente. Ten en cuenta que los LLM no son deterministas por naturaleza. Esto significa que incluso con la misma entrada, el modelo producir√° respuestas variables. Por lo tanto, tus resultados pueden variar ligeramente.

image::parasol-chat-response.webp[]

Lo que acabamos de hacer es proporcionar informaci√≥n contextual sobre la reclamaci√≥n en una conversaci√≥n con el LLM utilizando Prompt Engineering. Pero, por desgracia, el chatbot no sabe cu√°nto cuesta reparar un condensador de flujo, ni tendr√° ning√∫n conocimiento espec√≠fico del dominio de nuestra organizaci√≥n.

Con InstructLab, podemos cambiar eso ense√±ando al modelo.

[#entrenamiento]
== 2. Entrenamiento del Modelo

Hemos probado a chatear con el modelo y ahora vamos a aprovechar el potencial de InstructLab, centr√°ndonos en *mejorar la taxonom√≠a*. A√±adiremos conocimiento sobre InstructLab al modelo para que sepa m√°s del proyecto y pueda responder a nuestras preguntas. 

=== Entender la taxonom√≠a

¬øTe has preguntado por qu√© InstructLab se llama as√≠?

El *m√©todo LAB* (**L**arge-scale **A**lignment for chat**B**ots) se basa en taxonom√≠as. Las taxonom√≠as son archivos YAML que contienen conocimientos y habilidades que InstructLab usa para su generaci√≥n de datos. Estas se crean manualmente y con cuidado.

InstructLab facilita el proceso de ajuste y mejora de los modelos mediante la recopilaci√≥n de dos tipos de datos: conocimientos y habilidades. Esta informaci√≥n se recoge en una taxonom√≠a de archivos YAML que se usa en el proceso de generaci√≥n de datos sint√©ticos.

En la siguiente imagen puedes ver la estructura que puede tener una taxonom√≠a. Las cajas moradas son nuestros archivos YAML, o mejor dicho QNAs (archivos de preguntas y respuestas). Si desde ah√≠ seguimos hacia las capas de arriba, vemos que los YAML est√°n organizados por su tem√°tica: finanzas, matemasticas, etc... Mientras que si continuamos hacia abajo, vemos su papel en la generaci√≥n de datos sint√©ticos y entrenamiento.

image::taxonomy.png[]

=== Modificar la taxonom√≠a

Ahora partiendo de una taxonom√≠a vac√≠a, vamos a crear un lugar en el que almacenar informaci√≥n sobre el coche de McFly. Vuelve a la vista Terminales. En la ventana de terminal inferior donde hemos chateado, introduce 'exit' para salir de la sesi√≥n de chat.

[.console-input]
[source,bash]
----
mkdir -p /home/instruct/.local/share/instructlab/taxonomy/knowledge/parasol/claims
----

Ahora lo que nos falta es un archivo QNA con los detalles espec√≠ficos del coche. Por suerte, tenemos un PDF con esa informaci√≥n y podemos usar una herramienta open-source llamada Docling.

AAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAA

En el directorio ilab, ya hay preparado un archivo *qna.yaml*. InstructLab usa estos archivos para ense√±ar a los modelos. Estos contienen preguntas y respuestas sobre algo en concreto. Aqu√≠ tienes un ejemplo:

[source,bash]
----
- answer: The mission of instructlab is to let everyone shape generative AI
    by enabling contributed updates to existing LLMs in an accessible way.
    The community welcomes all those who would like to help enable everyone
    to shape the future of generative AI.
  question: 'What is the mission of Instructlab?'
----

Ahora vamos a incluir las preguntas y respuestas en el directorio que hemos creado.

[.console-input]
[source,bash]
----
cp qna.yaml ~/.local/share/instructlab/taxonomy/knowledge/instructlab/overview
----

Para comprobar que la sintaxis del *qna.yaml* es correcta, escribe el siguiente comando:

[.console-input]
[source,bash]
----
ilab taxonomy diff
----

Deber√≠as obtener lo siguiente:

[source,bash]
----
Taxonomy in /taxonomy/ is valid :)
----


=== Entrenar modelo

¬°Perfecto! Hemos a√±adido nuevo conocimiento en la taxonom√≠a. El siguiente paso es generar los datos sint√©ticos.

Un modelo maestro usar√° la taxonom√≠a que hemos definido para generar m√°s ejemplos de preguntas y respuestas. Cuantas m√°s preguntas y respuestas tengamos, m√°s s√≥lido ser√° el entrenamiento. Finalmente, entrenamos al modelo con nuestra taxonom√≠a y los datos sint√©ticos.
El resultado ser√° un nuevo modelo que comprenda el conocimiento que hemos indicado. Para m√°s informaci√≥n sobre el entrenamiento, visita este link:https://github.com/instructlab/instructlab?tab=readme-ov-file#-creating-new-knowledge-or-skills-and-training-the-model[link].

Generar datos sint√©ticos y entrenar lleva *varias horas* y por cuesti√≥n de tiempo, vamos a comprobar el aprendizaje usando el modelo preentrenado. ¬°Como si fuera un programa de cocina!

[#interaccion]
== 3. Comprobar modelo entrenado

¬°Hora de probar el modelo entrenado! Vamos al primer terminal y dejamos de servir el modelo usando `CTRL`+`C`. 

[source,bash]
----
INFO 2024-05-06 18:41:08,496 server.py:197 After application startup complete see http://127.0.0.1:8000/docs for API.
^C
Aborted!
----

Luego, servimos el modelo preentrenado:

[.console-input]
[source,bash]
----
ilab model serve --model-path ~/.cache/instructlab/models/ggml-ilab-pretrained-Q4_K_M.gguf
----

Esperamos unos segundos, volvemos al segundo terminal e iniciamos el chat con el LLM.


[.console-input]
[source,bash]
----
ilab model chat -m ~/.cache/instructlab/models/ggml-ilab-pretrained-Q4_K_M.gguf
----

¬°Llega la hora de la verdad! Prueba a preguntar al LLM sobre InstructLab:

[.console-input]
[source,bash]
----
Can you give me a short summary of what InstructLab is?
----

¬°Yuju! La respuesta deber√≠a ser mucho mejor que la √∫ltima vez. El LLM debe ser capaz de explicar que InstructLab.

=== Conclusi√≥n

*¬°Laboratorio terminado con √©xito!* Esperamos que hayas disfrutado probando de primera mano el potencial de InstructLab. Como peque√±o repaso, has conseguido lo siguiente:

* Chatear con un LLM
* Modificar la taxonom√≠a de InstructLab
* Comprobar el desempe√±o del modelo entrenado

Gracias por haber dedicado tu esfuerzo y tiempo a aprender m√°s sobre inteligencia articial y LLMs. Para m√°s informaci√≥n sobre InstructLab, ¬°echa un ojo a la comunidad en Github! https://github.com/instructlab


