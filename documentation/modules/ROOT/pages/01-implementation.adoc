= Guía del Laboratorio
include::_attributes.adoc[]

[#instalacion]
== 1. Instalación de InstructLab

=== Requisitos del sistema

* Python 3.9+
* 60GB de espacio en disco

=== Instalar el entorno

Para comenzar, establecemos un entorno virtual de Python que nos permitirá interactuar con InstructLab. Luego, instalaremos Instructlab.

[.console-input]
[source,bash]
----
cd ~/instructlab
source venv/bin/activate
pip install git+https://github.com/instructlab/instructlab.git@v0.17.1
----

¡Perfecto! Ya podemos usar Instructlab con el comando `ilab`.

[#uso-basico]
== 2. Uso básico de IntructLab

=== Inicialización

Antes de usar un modelo, hay que establecer la inicialización. Durante esta fase ocurren varias cosas:

* Se localiza una taxonomía por defecto en el sistema de archivos local.
* Se crea un archivo de configuración (config.yaml) en el directorio actual.

El archivo *config.yaml* contiene los valores predeterminados que utilizaremos. Estos nos permiten ajustar el comportamiento del modelo, por ejemplo que use un número determinado de CPUs.

[.console-input]
[source,bash]
----
ilab config init
-> Presiona enter cuando el comando pida argumentos.
----

El resultado final de ejecutar el comando es:

[source,bash]
----
'Initialization completed successfully, you`re ready to start using ilab. Enjoy!'
----

=== Descargar el modelo

Con el entorno configurado, ahora podemos descargar un modelo comprimido y optimizado al directorio local para ser utilizado como un servidor.

[.console-input]
[source,bash]
----
ilab model download --repository instructlab/granite-7b-lab-GGUF --filename=granite-7b-lab-Q4_K_M.gguf
----

El comando `ilab model download` descarga el modelo Granite 7b Lab desde los repositorios de HuggingFace. 

=== Servir el modelo

Ahora debemos servir el modelo para poder interactuar con él de forma muy parecida a como haríamos llamadas a una API.

[.console-input]
[source,bash]
----
ilab model serve --model-path models/granite-7b-lab-Q4_K_M.gguf
----

¡Genial! Con el modelo ya servido, estamos listos para probar el LLM.

=== Chatear con el modelo

Vamos a dejar el modelo sirviendose en el terminal donde hemos trabajado y abriremos una nueva pestaña del terminal. Volveremos a activar el entorno virtual de Python.

[.console-input]
[source,bash]
----
cd ~/instructlab
source venv/bin/activate
----

Ya dentro del entorno, podemos iniciar una sesión de chat con el comando ilab chat:

[.console-input]
[source,bash]
----
ilab model chat -m models/granite-7b-lab-Q4_K_M.gguf
----

En tu terminal debería aparecer:

[source,bash]
----
╭───────────────────────────────────────────────────────────────────────────╮
│ Welcome to InstructLab Chat w/ MODELS/GRANITE-7B-LAB-Q4_K_M.GGUF
╰───────────────────────────────────────────────────────────────────────────╯
>>>
----

Tenemos todo listo para hacerle preguntas a nuestro LLM. Prueba a escribir una pregunta: ¿Qué es Openshift en 20 palabras o menos?

[.console-input]
[source,bash]
----
¿Qué es Openshift en 20 palabras o menos?
----

¡Genial! Parece que el modelo funciona y es capaz de generar una respuesta a nuestras preguntas. 

[#entrenamiento]
== 3. Entrenamiento del Modelo

=== Preparar el entorno para el entrenamiento

Vamos a probar con otra pregunta como: 

[.console-input]
[source,bash]
----
¿Qué es InstructLab?
----

El modelo debería ser capaz de generar una respuesta que parece correcta, pero no lo es y se aleja mucho de la realidad.
Estos errores suelen denominarse «alucinaciones». La alineación de modelos (como la que haremos ahora) es una forma de mejorar las respuestas de un modelo y evitar alucinaciones.

Para solucionar esto, nos centraremos en añadir nuevo conocimiento al modelo para que sepa más sobre Instructlab. ¡Manos a la obra!

Primero vamos a terminar la sesión de chat con el modelo. Escribe 'exit'.

[.console-input]
[source,bash]
----
exit
----

Ahora vamos a mejorar el modelo modificando su taxonomía.

=== Entender la taxonomía

¿Te has preguntado por qué se llama InstructLab?

El método LAB se basa en taxonomías.
InstructLab facilita el proceso de ajuste y mejora de los modelos mediante la recopilación de dos tipos de datos: conocimientos y habilidades.
La comunidad open-source sube aportaciones y se recogen en una taxonomía de archivos YAML. Esta se utiliza en el proceso de generación de datos sintéticos. 


Echémosle un ojo a la taxonomía.

[.console-input]
[source,bash]
----
cd ~/instructlab
tree taxonomy/  | head
----

Podemos ver que la taxonomía recoge conocimiento sobre artes, ingeniería, geografía... Vamos a crear un directorio en el que insertar el conocimiento sobre InstructLab.

[.console-input]
[source,bash]
----
mkdir -p ~/instructlab/taxonomy/knowledge/instructlab/overview
----

Para este enfoque, proporcionamos un archivo llamado qna.yaml. Este contiene un cojunto de preguntas y respuestas sobre lo que queremos que aprenda el modelo.

-- EN CONSTRUCCION --


[#interaccion]
== 4. Interacción con el Modelo

=== Iniciar una sesión interactiva

Para comenzar una conversación con el modelo entrenado, usa el siguiente comando:

[.console-input]
[source,bash]
----
ilab model chat --model trained-model.pt
----

=== Evaluación de la salida

El modelo responderá en tiempo real a las entradas proporcionadas. Puedes usarlo para probar cómo responde a diferentes tipos de preguntas.

[#despliegue]
== 5. Despliegue del Modelo

=== Preparar el modelo para producción

Una vez entrenado, el modelo debe ser empaquetado para su despliegue. Usa el siguiente comando para empaquetar el modelo:

[.console-input]
[source,bash]
----
ilab package --model trained-model.pt --output model-package.tar.gz
----

=== Desplegar el modelo

El modelo empaquetado puede ser desplegado en servidores compatibles con InstructLab. Sube el archivo comprimido al servidor de producción y usa:

[.console-input]
[source,bash]
----
ilab deploy --package model-package.tar.gz --host production-server
----

