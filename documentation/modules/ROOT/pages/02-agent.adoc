include::_attributes.adoc[]

image::ramalama.png[]

= Laboratorio 2: Nuestro primer agente
 
Un agente es un sistema que puede tomar decisiones o responder preguntas de forma inteligente. Cuando hablamos de inteligencia artifical, las posibilidades se multiplican y podemos hacer que estos agentes lleguen a ocuparse de tareas muy específicas. De esta forma aceleramos nuestro proceso de trabajo y disponemos de herramientas aún más útiles.

En este segundo laboratorio, vamos a utilizar un modelo de lenguaje grande (LLM) e integrarlo en un agente de inteligencia artificial que nos permita hacer preguntas sobre documentos de nuestro dispositivo. Esto lo haremos gracias a la técnica RAG (Retrieval-Augmented Generation). El agente tomara nuestra pregunta, usará el archivo para obtener contexto y finalmente el modelo generará la respuesta más acorde.

[#entorno]
== 1. Configurar el entorno

Vamos a continuar trabajando en el terminal superior del laboratorio. Si estamos sirviendo un modelo, dejamos de ejecutarlo clicando en ese terminal y pulsando `CTRL`+`C` para obtener esto:

[source,bash]
----
INFO 2025-24-09 12:41:08,496 server.py:197 After application startup complete see http://127.0.0.1:8000/docs for API.
^C
Aborted!
----

Para empezar, debemos instalar link:https://instructlab.ai/[Ollama]. Esta es una herramienta que nos permite descargar una amplia variedad de modelos open-source y además poder ejecutarlos de forma local sacando partido a nuestro hardware. Simplemente ejecutamos en el terminal superior:

[.console-input]
[source,bash]
----
curl -fsSL https://ollama.com/install.sh | sh
----

Podrás ver cómo va la instalación con una barra de progreso. Una vez que esté al 100%, deberías recibir esto:

[source,bash]
----
######################################### 100.0%
>>> Creating ollama user...
>>> Adding ollama user to render group...
>>> Adding ollama user to video group...
>>> Adding current user to ollama gröup...
>>> Creating ollama systemd service...
>>> Enabling and starting ollama service...
Created symlink /etc/systemd/system/default.target.wants/ollama,service - /etc/systemd/system/olLama.service:
>>> NVIDIA GPU installed.
----

Ollama está ya instalado y ahora vamos a descargar el modelo LLama3.2 con 3 billones de parámetros. Un modelo bastante pequeño pero que funciona correctamente para este tipo de tareas:

[.console-input]
[source,bash]
----
ollama pull llama3.2:3b
----



Ya tenemos el modelo así que ahora está en nuestras manos desarrollar el agente. Lo más común es utilizar Python sobre todo por la enorme variedad de librerías especializadas en IA y por su simplicidad a la hora de programar. Vamos a descargar las librerías que nos hacen falta para ejecutar nuestro agente:

[.console-input]
[source,bash]
----
pip install sentence-transformers faiss-cpu
pip install PyPDF2
----

Estas librerías nos van a permitir realizar flujos RAG:


* `sentence-transformers`: Permite convertir textos (frases, párrafos, documentos) en vectores numéricos llamados embeddings.
* `faiss-cpu`: Para una búsqueda eficiente en grandes colecciones de vectores. Permite encontrar los vectores más cercanos a una consulta de forma muy rápida.
* `PyPDF2`: Permite leer, extraer texto y manipular archivos PDF desde Python.

Procedemos a descargar el código de Python:

[.console-input]
[source,bash]
----
mkdir /home/instruct/rag
curl -o /home/instruct/rag/rag_agent.py https://raw.githubusercontent.com/dgpmakes/instructlab-start/refs/heads/master/ilab/rag_agent.py
----

Vamos a echarle un ojo al código, puedes verlo con el siguiente comando:

[.console-input]
[source,bash]
----
cat /home/instruct/rag/rag_agent.py
----

[source,bash]
----
cat /home/instruct/rag/rag_agent.py
----

Python

Pillar archivo texto o PDF


[#agente]
== 2. Interactuar con el agente

python rag_agent.py <archivo>

Hacerle preguntas

== 3. Agentes en el mundo de la contenerización

Ramalama

Podman

Vamos a ver cómo sería el proceso. 

Documento NVIDIA

Repositorio de Quay de ramalama.



What are some recent examples of litigation faced by NVIDIA corporation?



